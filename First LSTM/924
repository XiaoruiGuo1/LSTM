import sys
import os
import math
from unicodedata import name
import numpy as np
import tensorflow as tf
from matplotlib import pyplot as plt
from keras import Input,losses
from keras.losses import Loss,MeanSquaredError
from keras.models import Model
from keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed,Layer
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam

# Create the dataset of subsystems
u = np.zeros(1800)
x_1 = np.zeros(1801)
x_2 = np.zeros(1801)
x_3 = np.zeros(1801)
x_4 = np.zeros(1801)
x_5 = np.zeros(1801)
x_6 = np.zeros(1801)
x_7 = np.zeros(1801)
x_8 = np.zeros(1801)
x_9 = np.zeros(1801)
x_10 = np.zeros(1801)
x_11 = np.zeros(1801)
x_12 = np.zeros(1801)
x_13 = np.zeros(1801)
x_14 = np.zeros(1801)
x_15 = np.zeros(1801)
x_16 = np.zeros(1801)
x_17 = np.zeros(1801)
x_18 = np.zeros(1801)
x_19 = np.zeros(1801)
x_20 = np.zeros(1801)

i = 0
for i in range(1800):
       u[i] = np.sin(i*np.pi/20) 
       x_1[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_2[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_3[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_4[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_5[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_6[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_7[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_8[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_9[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_10[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_11[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_12[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_13[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_14[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_15[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_16[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_17[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_18[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_19[i + 1] = 0.9*x_1[i] + 2*u[i]
       x_20[i + 1] = 0.9*x_1[i] + 2*u[i]
print(f"Shape of x_15:{x_15.shape}")

'''
# Visualize the dataset
h = np.arange(0,1801)
def u_plot (x_axis,y_axis,x_label,y_label):
    plt.figure(figsize = (18, 6))
    plt.plot(x_axis, y_axis, color ='black')
    plt.xlabel(x_label, {'fontsize': 12}) 
    plt.ylabel(y_label, {'fontsize': 12})
u_plot (h,x_1,'h','x_1')
plt.show()
'''

arrays_list = [x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10,
               x_11,x_12,x_13,x_14,x_15,x_16,x_17,x_18,x_19,x_20]


# Create the time series sequences
def time_series_sequence(input_dataset, output_dataset,input_length, output_length): 
    enc_U, enc_Y, dec_U, dec_Y = list(), list(), list(), list()
    for i in range(len(output_dataset)):
        enc_u = i + input_length
        enc_y = enc_u
        dec_u = enc_u + output_length - 1
        dec_y = enc_u + output_length
        if dec_y > len(output_dataset):
            break
        seq_enc_U, seq_enc_Y = input_dataset[i:enc_u].tolist(), output_dataset[i:enc_y].tolist()
        seq_dec_U, seq_dec_Y = input_dataset[enc_u-1:dec_u].tolist(),output_dataset[enc_y:dec_y]
        enc_U.append(seq_enc_U)
        enc_Y.append(seq_enc_Y)
        dec_U.append(seq_dec_U)
        dec_Y.append(seq_dec_Y)
    return np.array(enc_U), np.array(enc_Y), np.array(dec_U),np.array(dec_Y)


# Split the dataset into train and test
u = u.reshape(1800,1)
train_ind = 1000
y_train_list = []
y_test_list = []
for arrays in arrays_list:
    y_train = arrays[1:train_ind+1]
    y_test = arrays[train_ind+1:]
    y_train_list.append(y_train)
    y_test_list.append(y_test)

u_train = u[:train_ind]
u_test = u[train_ind:]

# Create the 3D train, test dataset to feed into LSTM layer.
input_length = 3 
output_length = 4
enc_Y_train_list = []
dec_Y_train_list = []
enc_Y_test_list = []
dec_Y_test_list = []
for arrays in y_train_list:
    enc_U_train, enc_Y_train,dec_U_train,dec_Y_train = time_series_sequence(u_train, arrays, input_length, output_length)
    enc_Y_train_list.append(enc_Y_train)
    dec_Y_train_list.append(dec_Y_train)
for arrays in y_test_list:
    enc_U_test, enc_Y_test, dec_U_test,dec_Y_test = time_series_sequence(u_test, arrays, input_length, output_length)
    enc_Y_test_list.append(enc_Y_test)
    dec_Y_test_list.append(dec_Y_test)

print(f"Shape of enc_U_train:{enc_U_train.shape}")
print(f"Shape of dec_U_test:{dec_U_test.shape}")
print(f"Shape of enc_Y_train_list[0]:{enc_Y_train_list[0].shape}")
print(f"Shape of dec_Y_test_list[0]:{dec_Y_test_list[0].shape}")
#If the system just has one output
enc_Y_train_1 = enc_Y_train_list[0]
dec_Y_train_1 = dec_Y_train_list[0]
z = np.dstack((enc_U_train,enc_Y_train_1))
print(f"Shape of z:{z.shape}")

'''
for i in range(2,20):
    combined_Y_train = np.dstack((combined_Y_train,Y_train_list[i]))
print(f"shape of combined_Y_train:{combined_Y_train.shape}")
Y_test_1 = Y_test_list[0]
Y_test_2 = Y_test_list[1]
combined_Y_test = np.dstack((Y_test_1,Y_test_2))
for i in range(2,20):
    combined_Y_test = np.dstack((combined_Y_test,Y_test_list[i]))
print(f"Shape of combined_Y_test:{combined_Y_test.shape}")
'''
# Put z into encoder below, get the hidden state
# and cell state. Then put dec_U_train into the
# decoder to predict.


'''
def Reduce_Dimensions_model():
    model = Sequential()
    #feedforward layer
    model.add(Input(shape=(5,20)))
    model.add(Dense(15,activation='linear'))
    model.add(Dense(5,activation='linear'))
    model.add(Dense(1,activation='linear',name="ffloutput"))#from 20D to 1D
    #LSTM encoder_decoder    
    #encoder
    model.add(LSTM(100, activation='sigmoid'))
    model.add(RepeatVector(5))
    #decoder
    model.add(LSTM(100,activation='sigmoid',return_sequences=True))
    model.add(TimeDistributed(Dense(units=1,activation = "linear"),name="finaloutput"))
    return model
model = Reduce_Dimensions_model()
model.summary()
'''

#Encoder-Decoder
class Encoder(Layer):
    def __init__(self,hiddens_num):
        super(Encoder, self).__init__()
        self.hiddens_num = hiddens_num
        self.lstm = LSTM(hiddens_num,return_sequences=True,return_state=True)

    def call(self,z):
        output = self.lstm(z,initial_state=None)
        states = output[1:]
        return output[0], states

class Decoder(Layer):
    def __init__(self,hiddens_num,Pred_num):
        super(Decoder, self).__init__()
        self.hiddens_num = hiddens_num
        self.lstm = LSTM(hiddens_num,return_sequences=True,return_state=True)
        self.Dense = Dense(Pred_num)
    
    def init_state(self,states):
        return states[1]

    def call(self,u,states):
        output = self.lstm(u,states)
        y = self.Dense(output[0])
        return y

class EncoderDecoder(Model):
    def __init__(self,encoder,decoder):
        super(EncoderDecoder,self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def call(self,input):
        Z, dec_U = input
        enc_outputs = self.encoder(Z)
        dec_state = self.decoder.init_state(enc_outputs)
        return self.decoder(dec_U,dec_state)

'''
def RDloss(dec_Y,dec_output):
    loss = MeanSquaredError()
    return loss(dec_Y,dec_output)
'''

encoder = Encoder(10)
decoder = Decoder(10,1)
encoderdecoder = EncoderDecoder(encoder,decoder)
encoderdecoder.compile(
          loss = losses.MeanSquaredError(),
          optimizer = Adam()
)
input = (z,dec_U_train)
encoderdecoder.fit(input,dec_Y_train_1,batch_size=10, epochs=50)



'''
# Train and test

Train = model.fit(z,dec_Y_train_1)

Y_predict = model.predict(U_test)
Y_pred = np.squeeze(Y_predict, axis=2)
Y_pred = Y_pred.flatten()
Y_test = np.squeeze(Y_test,axis=2)
Y_test = Y_test.flatten()


# Visualize the results
fig, ax = plt.subplots(1, 1, figsize=(15, 8))
ax.plot(Y_test, lw=3, c='y', label='test data')
ax.plot(Y_pred, lw=4, c='r',linestyle = ':', label='predictions')
ax.legend(loc="lower left")
plt.show();
'''